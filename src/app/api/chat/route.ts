import { NextRequest, NextResponse } from 'next/server';
import { MIN_SAMPLE, MAX_SAMPLE, MIN_THINKING, MAX_THINKING } from '@/config/chatConfig';
import { viewerPersonas } from '@/config/viewerConfig';
import { groq } from '@/lib/external/groq';
import { z } from 'zod';

const ChatResponseSchema = z.object({
  thoughts: z.array(z.string().describe(`A step of reflective thought on the current stream frame. `)).min(MIN_THINKING).max(MAX_THINKING).describe(`Array of ${MIN_THINKING} to ${MAX_THINKING} steps of reflective thought on the current stream frame`),
  interest_level: z.enum(['low', 'mid', 'high']).describe('Interest level of the persona regarding the stream: low, mid, or high'),
  chat: z.string().describe('The actual chat message in a streaming platform-like content with short and meme-like text snippet generated by the persona'),
}).describe('Schema for validating the modelâ€™s chat response JSON');

type ChatResponse = z.infer<typeof ChatResponseSchema>;

function getRandomInt(min: number, max: number): number {
  return Math.floor(Math.random() * (max - min + 1)) + min;
}

function sampleIndices(count: number, maxIndex: number): number[] {
  const indices = new Set<number>();
  while (indices.size < count) {
    indices.add(getRandomInt(0, maxIndex));
  }
  return Array.from(indices);
}

export async function POST(request: NextRequest) {
  try {
    const { images, currentMemory = "" } = await request.json();
    if (!images || !Array.isArray(images) || images.length === 0) {
      return NextResponse.json({ error: 'No images provided' }, { status: 400 });
    }

    const sampleCount = getRandomInt(MIN_SAMPLE, MAX_SAMPLE);
    const maxIndex = viewerPersonas.length - 1;
    const indices = sampleIndices(sampleCount, maxIndex);

    // Generate persona chat responses with memory context
    const personaPromises = indices.map(async (idx) => {
      const persona = viewerPersonas[idx];
      const messages = [
        {
          role: 'system',
          content: `You are ${persona.personality}.
            The following is a memory of what has been seen in the video stream so far:
            ${currentMemory || "No prior observations yet."}
            
            As you watch frames from a live stream, respond ONLY with valid JSON matching this schema:
            {
            "thoughts": ["string"], // between ${MIN_THINKING} and ${MAX_THINKING} items
            "interest_level": "low" | "mid" | "high",
            
            "chat": "string" (avoid asking questions too much. The chat message must look like a typical streaming platform chat message)
            }`
        },
        {
          role: 'user',
          content: [
            {
              type: "text",
              text: "Use these frames."
            },
            // Pass images as structured array content
            ...images.map((image: string) => ({
              type: 'image_url',
              image_url: {
                url: image
              }
            })),
          ],
        },
      ];
      const completion = await groq.chat.completions.create({
        model: 'meta-llama/llama-4-scout-17b-16e-instruct',
        response_format: { type: 'json_object' },
        messages: messages as any,
        temperature: 1.5,
        max_completion_tokens: 500,
      });
      const raw = completion.choices[0]?.message?.content;
      let parsed: ChatResponse;
      try {
        const json = JSON.parse(raw || '');
        parsed = ChatResponseSchema.parse(json);
      } catch (error) {
        console.error('JSON or schema validation failed', error);
        parsed = { interest_level: 'low', thoughts: [], chat: '' };
      }
      return { ...persona, ...parsed };
    });

    // Update memory based on new frames and existing memory
    const memoryPromise = (async () => {
      const memoryMessages = [
        {
          role: 'system',
          content: `You are a continuous memory builder for a video stream. Your task is to maintain a running memory of what has been observed in the stream over time, organized by scenes.

          Each scene represents a continuous view showing the same objects, people, or activities. If the past scenes are describing the similar person or the view compared to the new frames you received, you must assume it's the same scene. If so, update the existing scene by adding any new observation or events. When there's a change in view or what's being shown on camera, you should start a new paragraph in the memory. 

          Label each sceme as scene 1, scene 2,... where the new scene is with larger number. So scene 2 is more recent than scene 1. Do not change old scenes. Either update the latest scene or add a new scene to the string.

          Here is the current memory of what's been observed so far:
          ${currentMemory || "No observations yet."}

          Analyze the 5 new frames from the video stream and update the memory by:
          1. If there's a scene change, add a new paragraph describing the new scene
          2. If it's the same scene, update the existing paragraph with new observations
          3. Avoid repeating information if no scene change or significant event has occurred
          4. Be concise but descriptive, focusing on key details visible in the frames

          Your output will be the complete updated memory text, maintaining all past observations plus the new information.`
        },
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: `Please analyze these new frames and update the memory accordingly.`
            },
            ...images.map((image: string) => ({
              type: 'image_url',
              image_url: { url: image },
            })),
          ],
        },
      ];
      const memoryCompletion = await groq.chat.completions.create({
        model: 'meta-llama/llama-4-maverick-17b-128e-instruct',
        messages: memoryMessages as any,
        temperature: 0.7,
        max_completion_tokens: 500,
      });
      const updatedMemory = memoryCompletion.choices[0]?.message?.content || '';
      return { type: 'memory', indexCount: indices.length, memory: updatedMemory };
    })();

    // Execute all calls in parallel (persona responses + memory update)
    const allResults = await Promise.all([...personaPromises, memoryPromise]);

    // Separate persona chat results and memory
    const personaResults = (allResults as any[])
      .slice(0, indices.length);
    const memoryResult = allResults[allResults.length - 1];

    return NextResponse.json({ responses: personaResults, memory: memoryResult });
  } catch (err: any) {
    return NextResponse.json({ error: err.message }, { status: 500 });
  }
}
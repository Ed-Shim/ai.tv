import { NextRequest, NextResponse } from 'next/server';
import { MIN_SAMPLE, MAX_SAMPLE, MIN_THINKING, MAX_THINKING } from '@/config/chatConfig';
import { viewerPersonas } from '@/config/viewerConfig';
import { groq } from '@/lib/external/groq';
import { z } from 'zod';

const ChatResponseSchema = z.object({
  thoughts: z.array(z.string().describe(`A step of reflective thought on the current stream frame. `)).min(MIN_THINKING).max(MAX_THINKING).describe(`Array of ${MIN_THINKING} to ${MAX_THINKING} steps of reflective thought on the current stream frame`),
  interest_level: z.enum(['low', 'mid', 'high']).describe('Interest level of the persona regarding the stream: low, mid, or high'),
  is_sending_chat: z.boolean().describe('Whether the persona decides to send a chat message'),
  chat: z.string().describe('The actual chat message content generated by the persona'),
}).describe('Schema for validating the modelâ€™s chat response JSON');

type ChatResponse = z.infer<typeof ChatResponseSchema>;

function getRandomInt(min: number, max: number): number {
  return Math.floor(Math.random() * (max - min + 1)) + min;
}

function sampleIndices(count: number, maxIndex: number): number[] {
  const indices = new Set<number>();
  while (indices.size < count) {
    indices.add(getRandomInt(0, maxIndex));
  }
  return Array.from(indices);
}

export async function POST(request: NextRequest) {
  try {
    const { images } = await request.json();
    if (!images || !Array.isArray(images) || images.length === 0) {
      return NextResponse.json({ error: 'No images provided' }, { status: 400 });
    }

    const sampleCount = getRandomInt(MIN_SAMPLE, MAX_SAMPLE);
    const maxIndex = viewerPersonas.length - 1;
    const indices = sampleIndices(sampleCount, maxIndex);

    // Generate persona chat responses
    const personaPromises = indices.map(async (idx) => {
      const persona = viewerPersonas[idx];
      const messages = [
        {
          role: 'system',
          content: `You are ${persona.personality}.
            As you watch frames from a live stream, respond ONLY with valid JSON matching this schema:
            {
            "thoughts": ["string"], // between ${MIN_THINKING} and ${MAX_THINKING} items
            "interest_level": "low" | "mid" | "high",
            "is_sending_chat": boolean,
            "chat": "string"
            }`
        },
        {
          role: 'user',
          content: [
            {
              type: "text",
              text: "Use these frames."
            },
            // Pass images as structured array content
            ...images.map((image: string) => ({
              type: 'image_url',
              image_url: {
                url: image
              }
            })),
          ],
        },
      ];
      const completion = await groq.chat.completions.create({
        model: 'meta-llama/llama-4-scout-17b-16e-instruct',
        response_format: { type: 'json_object' },
        messages: messages as any,
        temperature: 1.0,
        max_completion_tokens: 500,
      });
      const raw = completion.choices[0]?.message?.content;
      let parsed: ChatResponse;
      try {
        const json = JSON.parse(raw || '');
        parsed = ChatResponseSchema.parse(json);
      } catch (error) {
        console.error('JSON or schema validation failed', error);
        parsed = { interest_level: 'low', thoughts: [], is_sending_chat: false, chat: '' };
      }
      return { ...persona, ...parsed };
    });

    // Generate summary memory for sampled frames
    const summaryPromise = (async () => {
      const summaryMessages = [
        {
          role: 'system',
          content: 'You are a summarization assistant. Given several frames from a live video, generate a short descriptive memory summarizing what happened over these frames.'
        },
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: `Please summarize the events in the following frames`
            },
            ...images.map((image: string) => ({
              type: 'image_url',
              image_url: { url: image },
            })),
          ],
        },
      ];
      const summaryCompletion = await groq.chat.completions.create({
        model: 'meta-llama/llama-4-maverick-17b-128e-instruct',
        messages: summaryMessages as any,
        temperature: 1.0,
        max_completion_tokens: 100,
      });
      const memory = summaryCompletion.choices[0]?.message?.content || '';
      return { type: 'memory', indexCount: indices.length, memory };
    })();

    // Execute all calls in parallel (persona responses + summary)
    const allResults = await Promise.all([...personaPromises, summaryPromise]);

    // Separate persona chat results and summary
    const personaResults = (allResults as any[])
      .slice(0, indices.length)
      // .filter(res => res.is_sending_chat);
    const summaryResult = allResults[allResults.length - 1];

    return NextResponse.json({ responses: personaResults, memory: summaryResult });
  } catch (err: any) {
    return NextResponse.json({ error: err.message }, { status: 500 });
  }
}